# -*- coding: utf-8 -*-
"""udemy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-13kjZNjWpZkH9k26joSxx-j7LEjVKAz
"""

# importing necessary libs:
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem.porter import PorterStemmer

#uploading the kaggle dataset for udemy courses
from google.colab import files
files.upload()

!mkdir -p ~/.dataset_task0
!cp kaggle.json ~/.dataset_task0/

!chmod 600 ~/.dataset_task0/kaggle.json

!pip install kaggle

!kaggle datasets download -d andrewmvd/udemy-courses

!unzip udemy-courses.zip

# exploratory data analysis
df_1 = pd.read_csv('udemy_courses.csv')

df = df_1[['course_title', 'is_paid', 'level', 'subject']]
filtered_df = df[df['is_paid'] == False]

# preprocessing text:
nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = text.strip()
        text = text.translate(str.maketrans('', '', string.punctuation))
        tokens = nltk.word_tokenize(text)
        stop_words = set(stopwords.words('english'))
        tokens = [word for word in tokens if word not in stop_words]
        text = ' '.join(tokens)
    return text

columns_to_preprocess = ['course_title', 'level', 'subject']
for column in columns_to_preprocess:
    if column in filtered_df.columns:
        filtered_df[column] = filtered_df[column].apply(preprocess_text)

filtered_df['new_col'] = filtered_df['course_title'] + ' ' + filtered_df['level'] + ' ' + filtered_df['subject']

ps = PorterStemmer()
def stem(text):
    return " ".join([ps.stem(word) for word in text.split()])

filtered_df['new_col'] = filtered_df['new_col'].apply(stem)

# creating vectors for course similarity
cv = CountVectorizer(max_features=5000, stop_words='english')
vectors = cv.fit_transform(filtered_df['new_col']).toarray()

similarity = cosine_similarity(vectors)

# recommendation function:
def recommend(course):
    try:
        preprocessed_course = preprocess_text(course)
        stemmed_course = stem(preprocessed_course)

        matches = filtered_df[filtered_df['new_col'].str.contains(stemmed_course, regex=False, na=False)]
        if not matches.empty:
            course_index = matches.index[0]
            distances = similarity[course_index]
            course_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:7]

            print("Recommended courses:")
            for i in course_list:
                print(df_1.iloc[i[0]]['course_title'])
        else:
            print("Course not found. Please check the course title.")

    except IndexError:
        print("Course not found. Please check the course title.")

# trying an example to test the model:
recommend('Trading')

# exporting the model:
import pickle

with open('udemy_vectorizer.pkl', 'wb') as file:
    pickle.dump(cv, file)

with open('udemy_similarity.pkl', 'wb') as file:
    pickle.dump(similarity, file)